{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6143689d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import Sampler\n",
    "from PIL import Image, ImageOps\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch.nn.functional as F\n",
    "from resnest.torch import resnest50\n",
    "from NLBlock_TVL import TVL,NLBlock\n",
    "\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "from torchvision.transforms import Lambda\n",
    "import argparse\n",
    "import copy\n",
    "import random\n",
    "import numbers\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn import metrics\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4559e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available()==True:\n",
    "    device=\"cuda:2\"\n",
    "else:\n",
    "    device =\"cpu\"\n",
    "    \n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fbc00a",
   "metadata": {},
   "source": [
    "# 1. Models:M4, M5, M6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c59e67fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 4 CataRCNet: resnet + lstm + non-local \n",
    "class M4_resnet_lstm_nl(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(M4_resnet_lstm_nl, self).__init__()\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        self.share = torch.nn.Sequential()\n",
    "        self.share.add_module(\"conv1\", resnet.conv1)\n",
    "        self.share.add_module(\"bn1\", resnet.bn1)\n",
    "        self.share.add_module(\"relu\", resnet.relu)\n",
    "        self.share.add_module(\"maxpool\", resnet.maxpool)\n",
    "        self.share.add_module(\"layer1\", resnet.layer1)\n",
    "        self.share.add_module(\"layer2\", resnet.layer2)\n",
    "        self.share.add_module(\"layer3\", resnet.layer3)\n",
    "        self.share.add_module(\"layer4\", resnet.layer4)\n",
    "        self.share.add_module(\"avgpool\", resnet.avgpool)\n",
    "        self.lstm = nn.LSTM(2048, 512, batch_first=True)\n",
    "        self.fc_c = nn.Linear(512, 19) #7\n",
    "        self.fc_h_c = nn.Linear(1024, 512)\n",
    "        self.nl_block = NLBlock()\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "        init.xavier_normal_(self.lstm.all_weights[0][0])\n",
    "        init.xavier_normal_(self.lstm.all_weights[0][1])\n",
    "        init.xavier_uniform_(self.fc_c.weight)\n",
    "        init.xavier_uniform_(self.fc_h_c.weight)\n",
    "\n",
    "    def forward(self, x, long_feature=None):\n",
    "        x = x.view(-1, 3, 216,216)\n",
    "        x = self.share.forward(x)\n",
    "        x = x.view(-1, sequence_length, 2048)\n",
    "        self.lstm.flatten_parameters()\n",
    "        y, _ = self.lstm(x)\n",
    "        y = y.contiguous().view(-1, 512)\n",
    "        y = y[sequence_length - 1::sequence_length]\n",
    "\n",
    "        y_1 = self.nl_block(y, long_feature)\n",
    "        y = torch.cat([y, y_1], dim=1)\n",
    "        y = self.dropout(self.fc_h_c(y))\n",
    "        y = F.relu(y)\n",
    "        y = self.fc_c(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a1e55bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# M5: densenet + lstm + non-local\n",
    "class M5_densenet_lstm_nl(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(M5_densenet_lstm_nl, self).__init__()\n",
    "        densenet = models.densenet169(pretrained=True) #pretrained=True\n",
    "        self.share = torch.nn.Sequential()\n",
    "        self.share.add_module(\"features\", densenet.features)\n",
    "        #self.share.add_module(\"avgpool\", resnet.avgpool)\n",
    "        self.avg = nn.AvgPool2d(6)\n",
    "\n",
    "        # self.share.add_module(\"classifier\", resnet.classifier)\n",
    "        \n",
    "        #self.fc_1 = nn.Linear(9216, 4096)\n",
    "         \n",
    "        self.lstm = nn.LSTM(1664, 512, batch_first=True)\n",
    "#        self.lstm = nn.LSTM(2028, 512, batch_first=True)\n",
    "        #self.fc = nn.Linear(512, 19)\n",
    "        self.fc_c = nn.Linear(512, 19) #7\n",
    "        self.fc_h_c = nn.Linear(1024, 512)\n",
    "        self.nl_block = NLBlock()\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "        \n",
    "        init.xavier_normal_(self.lstm.all_weights[0][0])\n",
    "        init.xavier_normal_(self.lstm.all_weights[0][1])\n",
    "        init.xavier_uniform_(self.fc_c.weight)\n",
    "        init.xavier_uniform_(self.fc_h_c.weight)\n",
    "\n",
    "    def forward(self, x,long_feature):\n",
    "        x = x.view(-1, 3, 216, 216)\n",
    "        x = self.share.forward(x) # ([100, 1664, 6, 6])   # ([100,2048,1,1])\n",
    "        x = self.avg(x)\n",
    "        x = x.view(-1, sequence_length, 1664)  \n",
    "        self.lstm.flatten_parameters()\n",
    "        y, _ = self.lstm(x)\n",
    "        y = y.contiguous().view(-1, 512)\n",
    "        y = y[sequence_length - 1::sequence_length]\n",
    "        \n",
    "        \n",
    "        y_1 = self.nl_block(y, long_feature)\n",
    "        y = torch.cat([y, y_1], dim=1)\n",
    "        y = self.dropout(self.fc_h_c(y))\n",
    "        y = F.relu(y)\n",
    "        y = self.fc_c(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40110887",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fceb273a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 6  resnest + lstm + non-local \n",
    "class M6_resnest_lstm_nl(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(M6_resnest_lstm_nl, self).__init__()\n",
    "        resnet = resnest50(pretrained=True)\n",
    "        self.share = torch.nn.Sequential()\n",
    "        self.share.add_module(\"conv1\", resnet.conv1)\n",
    "        self.share.add_module(\"bn1\", resnet.bn1)\n",
    "        self.share.add_module(\"relu\", resnet.relu)\n",
    "        self.share.add_module(\"maxpool\", resnet.maxpool)\n",
    "        self.share.add_module(\"layer1\", resnet.layer1)\n",
    "        self.share.add_module(\"layer2\", resnet.layer2)\n",
    "        self.share.add_module(\"layer3\", resnet.layer3)\n",
    "        self.share.add_module(\"layer4\", resnet.layer4)\n",
    "        self.share.add_module(\"avgpool\", resnet.avgpool)\n",
    "        self.lstm = nn.LSTM(2048, 512, batch_first=True)\n",
    "        self.fc_c = nn.Linear(512, 19) #7\n",
    "        self.fc_h_c = nn.Linear(1024, 512)\n",
    "        self.nl_block = NLBlock()\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "        init.xavier_normal_(self.lstm.all_weights[0][0])\n",
    "        init.xavier_normal_(self.lstm.all_weights[0][1])\n",
    "        init.xavier_uniform_(self.fc_c.weight)\n",
    "        init.xavier_uniform_(self.fc_h_c.weight)\n",
    "\n",
    "    def forward(self, x, long_feature=None):\n",
    "        x = x.view(-1, 3, 216,216)\n",
    "        x = self.share.forward(x)\n",
    "        x = x.view(-1, sequence_length, 2048)\n",
    "        self.lstm.flatten_parameters()\n",
    "        y, _ = self.lstm(x)\n",
    "        y = y.contiguous().view(-1, 512)\n",
    "        y = y[sequence_length - 1::sequence_length]\n",
    "\n",
    "        y_1 = self.nl_block(y, long_feature)\n",
    "        y = torch.cat([y, y_1], dim=1)\n",
    "        y = self.dropout(self.fc_h_c(y))\n",
    "        y = F.relu(y)\n",
    "        y = self.fc_c(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e1fcaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "M4 = M4_resnet_lstm_nl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b32c570f",
   "metadata": {},
   "outputs": [],
   "source": [
    "M5 = M5_densenet_lstm_nl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6ce88b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "M6 = M6_resnest_lstm_nl()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7036d9a3",
   "metadata": {},
   "source": [
    "# 2. LFB Models: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d15267b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#long feature bank bank\n",
    "\n",
    "# resnet+lstm lfb  used for M4, M7  \n",
    "class LFB_resnet_lstm(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LFB_resnet_lstm, self).__init__()\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        self.share = torch.nn.Sequential()\n",
    "        self.share.add_module(\"conv1\", resnet.conv1)\n",
    "        self.share.add_module(\"bn1\", resnet.bn1)\n",
    "        self.share.add_module(\"relu\", resnet.relu)\n",
    "        self.share.add_module(\"maxpool\", resnet.maxpool)\n",
    "        self.share.add_module(\"layer1\", resnet.layer1)\n",
    "        self.share.add_module(\"layer2\", resnet.layer2)\n",
    "        self.share.add_module(\"layer3\", resnet.layer3)\n",
    "        self.share.add_module(\"layer4\", resnet.layer4)\n",
    "        self.share.add_module(\"avgpool\", resnet.avgpool)\n",
    "        self.lstm = nn.LSTM(2048, 512, batch_first=True)\n",
    "\n",
    "        init.xavier_normal_(self.lstm.all_weights[0][0])\n",
    "        init.xavier_normal_(self.lstm.all_weights[0][1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 3, 216,216)\n",
    "        x = self.share.forward(x)\n",
    "        x = x.view(-1, sequence_length, 2048)\n",
    "        self.lstm.flatten_parameters()\n",
    "        y, _ = self.lstm(x)\n",
    "        y = y.contiguous().view(-1, 512)\n",
    "        y = y[sequence_length - 1::sequence_length]\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1e00db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# long feature bank used for M5, M8\n",
    "class LFB_densenet_lstm(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LFB_densenet_lstm, self).__init__()\n",
    "        densenet = models.densenet169(pretrained=True) #pretrained=True\n",
    "        self.share = torch.nn.Sequential()\n",
    "        self.share.add_module(\"features\", densenet.features)\n",
    "        #self.share.add_module(\"avgpool\", resnet.avgpool)\n",
    "        self.avg = nn.AvgPool2d(6)\n",
    "\n",
    "        # self.share.add_module(\"classifier\", resnet.classifier)      \n",
    "        #self.fc_1 = nn.Linear(9216, 4096)\n",
    "        \n",
    "        # \n",
    "        self.lstm = nn.LSTM(1664, 512, batch_first=True)\n",
    "#        self.lstm = nn.LSTM(2028, 512, batch_first=True)\n",
    "        #self.fc = nn.Linear(512, 19)\n",
    "\n",
    "        \n",
    "\n",
    "        init.xavier_normal_(self.lstm.all_weights[0][0])\n",
    "        init.xavier_normal_(self.lstm.all_weights[0][1])\n",
    "        #init.xavier_uniform_(self.fc.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 3, 216, 216)\n",
    "        x = self.share.forward(x) # ([100, 1664, 6, 6])   # ([100,2048,1,1])\n",
    "        x = self.avg(x)\n",
    "        x = x.view(-1, sequence_length, 1664)  \n",
    "        self.lstm.flatten_parameters()\n",
    "        y, _ = self.lstm(x)\n",
    "        y = y.contiguous().view(-1, 512)\n",
    "        y = y[sequence_length - 1::sequence_length]\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7e9b5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#long feature bank bank\n",
    "\n",
    "# resnet+lstm lfb  used for M6,M9\n",
    "class LFB_resnest_lstm(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LFB_resnest_lstm, self).__init__()\n",
    "        resnest = resnest50(pretrained=True)\n",
    "        self.share = torch.nn.Sequential()\n",
    "        self.share.add_module(\"conv1\", resnest.conv1)\n",
    "        self.share.add_module(\"bn1\", resnest.bn1)\n",
    "        self.share.add_module(\"relu\", resnest.relu)\n",
    "        self.share.add_module(\"maxpool\", resnest.maxpool)\n",
    "        self.share.add_module(\"layer1\", resnest.layer1)\n",
    "        self.share.add_module(\"layer2\", resnest.layer2)\n",
    "        self.share.add_module(\"layer3\", resnest.layer3)\n",
    "        self.share.add_module(\"layer4\", resnest.layer4)\n",
    "        self.share.add_module(\"avgpool\", resnest.avgpool)\n",
    "        self.lstm = nn.LSTM(2048, 512, batch_first=True)\n",
    "\n",
    "        init.xavier_normal_(self.lstm.all_weights[0][0])\n",
    "        init.xavier_normal_(self.lstm.all_weights[0][1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 3, 216,216)\n",
    "        x = self.share.forward(x)\n",
    "        x = x.view(-1, sequence_length, 2048)\n",
    "        self.lstm.flatten_parameters()\n",
    "        y, _ = self.lstm(x)\n",
    "        y = y.contiguous().view(-1, 512)\n",
    "        y = y[sequence_length - 1::sequence_length]\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f48b24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36af4bc8",
   "metadata": {},
   "source": [
    "# 3. Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d63e6e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedCrossEntropy(torch.nn.Module):\n",
    "    '''\n",
    "    WCE\n",
    "    '''       \n",
    "     # 6-25\n",
    "    def __init__(self, weight=torch.Tensor([0.0033, 0.4182, 0.1321, 0.0234, 0.0344, 0.0146, 0.0428, 0.0140, 0.0092,\n",
    "        0.0272, 0.0096, 0.0323, 0.0341, 0.0508, 0.0151, 0.0160, 0.0365, 0.0738,\n",
    "        0.0128])):\n",
    "        super(WeightedCrossEntropy, self).__init__()\n",
    "        \n",
    "        weight = weight.to(device)\n",
    "        self.weighted_cross_entropy = nn.CrossEntropyLoss(weight=weight)\n",
    "        \n",
    "    def forward(self, inputs, target):\n",
    "        return self.weighted_cross_entropy.forward(inputs, target)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5d22b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7feaa1ed",
   "metadata": {},
   "source": [
    "# 4. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7db02ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pil_loader(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        with Image.open(f) as img:\n",
    "            return img.convert('RGB')\n",
    "\n",
    "class CataractsDataset(Dataset):\n",
    "    def __init__(self, file_paths,file_labels, transform=None,loader=pil_loader):\n",
    "        self.file_paths = file_paths\n",
    "        self.file_labels_phase = file_labels[:,0]\n",
    "        self.transform = transform\n",
    "        self.loader = loader\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_names = self.file_paths[index]\n",
    "        labels = self.file_labels_phase[index]\n",
    "        imgs = self.loader(img_names)\n",
    "        if self.transform is not None:\n",
    "            imgs = self.transform(imgs)\n",
    "\n",
    "        return imgs, labels, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "263ea59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(data_path):\n",
    "    with open(data_path, 'rb') as f:\n",
    "        train_test_paths_labels = pickle.load(f)\n",
    "    train_paths_50 = train_test_paths_labels[0]\n",
    "    val_paths_50 = train_test_paths_labels[1]\n",
    "    train_labels_50 = train_test_paths_labels[2]\n",
    "    val_labels_50 = train_test_paths_labels[3]\n",
    "    train_num_each_50 = train_test_paths_labels[4]\n",
    "    val_num_each_50 = train_test_paths_labels[5]\n",
    "\n",
    "    print('train_paths_20  : {:6d}'.format(len(train_paths_50)))\n",
    "    print('train_labels_20 : {:6d}'.format(len(train_labels_50)))\n",
    "    print('valid_paths_5  : {:6d}'.format(len(val_paths_50)))\n",
    "    print('valid_labels_5 : {:6d}'.format(len(val_labels_50)))\n",
    "\n",
    "    # train_labels_19 = np.asarray(train_labels_19, dtype=np.int64) yilin comment\n",
    "    train_labels_50 = np.asarray(train_labels_50, dtype=np.int64)\n",
    "    val_labels_50 = np.asarray(val_labels_50, dtype=np.int64)\n",
    "    \n",
    "    train_transforms = transforms.Compose([\n",
    "            transforms.CenterCrop(216),\n",
    "            transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomRotation(5),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "    \n",
    "    test_transforms = transforms.Compose([\n",
    "            transforms.CenterCrop(216),\n",
    "            transforms.ToTensor(),\n",
    "\n",
    "        ])\n",
    "\n",
    "    train_dataset_50 = CataractsDataset(train_paths_50, train_labels_50, train_transforms)\n",
    "    val_dataset_50 = CataractsDataset(val_paths_50, val_labels_50, test_transforms)\n",
    "    train_dataset_50_LFB =CataractsDataset(train_paths_50, train_labels_50, test_transforms)\n",
    "    \n",
    "    return (train_dataset_50,train_dataset_50_LFB), train_num_each_50,val_dataset_50, val_num_each_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f9aa09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef3552d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6790400c",
   "metadata": {},
   "source": [
    "# 4. Load long term feature bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "325e5cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Long Term Feature bank\n",
    "g_LFB_train = np.zeros(shape=(0, 512))\n",
    "g_LFB_val = np.zeros(shape=(0, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4361d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sliding window\n",
    "def get_start_idx(sequence_length, list_each_length):\n",
    "    count = 0\n",
    "    idx = []\n",
    "    for i in range(len(list_each_length)):\n",
    "        for j in range(count, count + (list_each_length[i] + 1 - sequence_length)):\n",
    "            idx.append(j)\n",
    "        count += list_each_length[i]\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b99a551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_long_feature(start_index_list, dict_start_idx_LFB, lfb):\n",
    "    long_feature = []\n",
    "    for j in range(len(start_index_list)):\n",
    "        long_feature_each = []\n",
    "        \n",
    "        # 上一个存在feature的index\n",
    "        last_LFB_index_no_empty = dict_start_idx_LFB[int(start_index_list[j])]\n",
    "        \n",
    "        # \n",
    "        for k in range(LFB_length):\n",
    "            LFB_index = (start_index_list[j] - k - 1)\n",
    "            if int(LFB_index) in dict_start_idx_LFB:                \n",
    "                LFB_index = dict_start_idx_LFB[int(LFB_index)]\n",
    "                long_feature_each.append(lfb[LFB_index])\n",
    "                last_LFB_index_no_empty = LFB_index\n",
    "            else:\n",
    "                long_feature_each.append(lfb[last_LFB_index_no_empty])\n",
    "            \n",
    "        long_feature.append(long_feature_each)\n",
    "    return long_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc6fd636",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceSampler(Sampler):\n",
    "    def __init__(self, data_source, idx):\n",
    "        super().__init__(data_source)\n",
    "        self.data_source = data_source\n",
    "        self.idx = idx\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dc8e09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df3a0878",
   "metadata": {},
   "source": [
    "# 5. Training M4, M5, M6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f40015d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Long Term Feature bank\n",
    "g_LFB_train = np.zeros(shape=(0, 512))\n",
    "g_LFB_val = np.zeros(shape=(0, 512))\n",
    "\n",
    "def train_model(model,train_dataset_2, train_num_each, val_dataset, val_num_each):\n",
    "    # TensorBoard\n",
    "    writer = SummaryWriter(tensorboard_path)\n",
    "    \n",
    "    (train_dataset,train_dataset_LFB) = train_dataset_2\n",
    "    \n",
    "    # choose start index for sequence \n",
    "    train_useful_start_idx = get_start_idx(sequence_length, train_num_each)\n",
    "    val_useful_start_idx = get_start_idx(sequence_length, val_num_each)\n",
    "\n",
    "    num_train_we_use = len(train_useful_start_idx)\n",
    "    num_val_we_use = len(val_useful_start_idx)\n",
    "    \n",
    "    # choose start index for feature bank\n",
    " \n",
    "    \n",
    "    train_useful_start_idx_LFB = train_useful_start_idx\n",
    "    val_useful_start_idx_LFB = val_useful_start_idx\n",
    "\n",
    "    num_train_we_use_LFB = num_train_we_use \n",
    "    num_val_we_use_LFB = num_val_we_use\n",
    "    \n",
    "    \n",
    "    train_idx = []\n",
    "    for i in range(num_train_we_use):\n",
    "        for j in range(sequence_length):\n",
    "            train_idx.append(train_useful_start_idx[i] + j)\n",
    "\n",
    "    val_idx = []\n",
    "    for i in range(num_val_we_use):\n",
    "        for j in range(sequence_length):\n",
    "            val_idx.append(val_useful_start_idx[i] + j)\n",
    "\n",
    "    num_train_all = len(train_idx)\n",
    "    num_val_all = len(val_idx)\n",
    "    \n",
    "    \n",
    "    train_idx_LFB = train_idx\n",
    "    val_idx_LFB = val_idx\n",
    "    \n",
    "    dict_index, dict_value = zip(*list(enumerate(train_useful_start_idx_LFB)))\n",
    "    dict_train_start_idx_LFB = dict(zip(dict_value, dict_index))\n",
    "\n",
    "    dict_index, dict_value = zip(*list(enumerate(val_useful_start_idx_LFB)))\n",
    "    dict_val_start_idx_LFB = dict(zip(dict_value, dict_index))\n",
    "\n",
    "    print('num train start idx : {:6d}'.format(len(train_useful_start_idx)))\n",
    "    print('num of all train use: {:6d}'.format(num_train_all))\n",
    "    print('num of all valid use: {:6d}'.format(num_val_all))\n",
    "    print('num of all train LFB use: {:6d}'.format(len(train_idx_LFB)))\n",
    "    print('num of all valid LFB use: {:6d}'.format(len(val_idx_LFB)))\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=val_batch_size,\n",
    "        sampler=SequenceSampler(val_dataset, val_idx),\n",
    "        num_workers=workers,\n",
    "        pin_memory=False\n",
    "    )\n",
    "\n",
    "    \n",
    "    # load long feature bank\n",
    "    global g_LFB_train\n",
    "    global g_LFB_val\n",
    "    print(\"loading features!->.........\")\n",
    "    \n",
    "    if not load_exist_LFB:\n",
    "        g_LFB_train = np.zeros(shape=(0, 512))\n",
    "        g_LFB_val = np.zeros(shape=(0, 512))\n",
    "        \n",
    "        train_feature_loader = DataLoader(\n",
    "            train_dataset_LFB,\n",
    "            batch_size=val_batch_size,\n",
    "            sampler=SequenceSampler(train_dataset_LFB, train_idx_LFB),\n",
    "            num_workers=workers,\n",
    "            pin_memory=False\n",
    "        )\n",
    "        val_feature_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=val_batch_size,\n",
    "            sampler=SequenceSampler(val_dataset, val_idx_LFB),\n",
    "            num_workers=workers,\n",
    "            pin_memory=False\n",
    "        )\n",
    "        \n",
    "        model_LFB.load_state_dict(torch.load(LFB_path), strict=False)\n",
    "        \n",
    "        model_LFB.to(device)\n",
    "        model_LFB.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "\n",
    "            for data in train_feature_loader:\n",
    "                inputs, labels_phase = data[0].to(device), data[1].to(device)\n",
    "\n",
    "\n",
    "                inputs = inputs.view(-1, sequence_length, 3, 216,216)\n",
    "                outputs_feature = model_LFB.forward(inputs)\n",
    "\n",
    "                for j in range(len(outputs_feature)):\n",
    "                    save_feature = outputs_feature.data.cpu()[j].numpy()\n",
    "                    save_feature = save_feature.reshape(1, 512)\n",
    "                    g_LFB_train = np.concatenate((g_LFB_train, save_feature),axis=0)\n",
    "\n",
    "                print(\"train feature length:\",len(g_LFB_train))\n",
    "\n",
    "            for data in val_feature_loader:\n",
    "                \n",
    "                inputs, labels_phase = data[0].to(device), data[1].to(device)\n",
    "\n",
    "\n",
    "                inputs = inputs.view(-1, sequence_length, 3, 216,216)\n",
    "                outputs_feature = model_LFB.forward(inputs)\n",
    "\n",
    "                for j in range(len(outputs_feature)):\n",
    "                    save_feature = outputs_feature.data.cpu()[j].numpy()\n",
    "                    save_feature = save_feature.reshape(1, 512)\n",
    "                    g_LFB_val = np.concatenate((g_LFB_val, save_feature), axis=0)\n",
    "\n",
    "                print(\"val feature length:\",len(g_LFB_val))\n",
    "\n",
    "        print(\"finish!\")\n",
    "        g_LFB_train = np.array(g_LFB_train)\n",
    "        g_LFB_val = np.array(g_LFB_val)\n",
    "\n",
    "        # LFB_train_path_save_path = \"./LFB/g_LFB_train_densenet.pkl\"\n",
    "        with open(LFB_train_path_save_path, 'wb') as f:\n",
    "            pickle.dump(g_LFB_train, f)\n",
    "\n",
    "        with open(LFB_val_path_save_path, 'wb') as f:\n",
    "            pickle.dump(g_LFB_val, f)\n",
    "    \n",
    "    else:\n",
    "        with open(LFB_train_path_save_path, 'rb') as f:\n",
    "            g_LFB_train = pickle.load(f)\n",
    "\n",
    "        with open(LFB_val_path_save_path, 'rb') as f:\n",
    "            g_LFB_val = pickle.load(f)\n",
    "\n",
    "        print(\"load completed\")\n",
    "        \n",
    "        \n",
    "    print(\"g_LFB_train shape:\",g_LFB_train.shape)\n",
    "    print(\"g_LFB_val shape:\",g_LFB_val.shape)\n",
    "        \n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    #####################################\n",
    "    #model = resnet_lstm()\n",
    "    # model.load_state_dict(torch.load(pretrained_model_path))\n",
    "    #####################################   \n",
    "    \n",
    "    model.load_state_dict(torch.load(LFB_path), strict=False)\n",
    "    model.to(device)\n",
    "\n",
    "    criterion_phase = WeightedCrossEntropy() #nn.CrossEntropyLoss(size_average=False)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict()) \n",
    "    \n",
    "    best_val_accuracy_phase = 0.0\n",
    "    correspond_train_acc_phase = 0.0\n",
    "    best_epoch = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        np.random.shuffle(train_useful_start_idx)\n",
    "        train_idx = []\n",
    "        for i in range(num_train_we_use):\n",
    "            for j in range(sequence_length):\n",
    "                train_idx.append(train_useful_start_idx[i] + j)\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=train_batch_size,\n",
    "            sampler=SequenceSampler(train_dataset, train_idx),\n",
    "            num_workers=workers,\n",
    "            pin_memory=False\n",
    "        )\n",
    "\n",
    "        # in training mode.\n",
    "        model.train()\n",
    "        train_loss_phase = 0.0\n",
    "        train_corrects_phase = 0\n",
    "        batch_progress = 0.0\n",
    "        running_loss_phase = 0.0\n",
    "        minibatch_correct_phase = 0.0\n",
    "        train_start_time = time.time()\n",
    "        for i, data in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            inputs, labels_phase = data[0].to(device), data[1].to(device)\n",
    "\n",
    "            labels_phase = labels_phase[(sequence_length - 1)::sequence_length]\n",
    "\n",
    "            start_index_list = data[2]\n",
    "            start_index_list = start_index_list[0::sequence_length]\n",
    "            long_feature = get_long_feature(start_index_list=start_index_list,\n",
    "                                            dict_start_idx_LFB=dict_train_start_idx_LFB,\n",
    "                                            lfb=g_LFB_train)\n",
    "\n",
    "            long_feature = (torch.Tensor(long_feature)).to(device)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            inputs = inputs.view(-1, sequence_length, 3, 216,216) #224, 224)\n",
    "            outputs_phase = model.forward(inputs,long_feature=long_feature)\n",
    "            #outputs_phase = outputs_phase[sequence_length - 1::sequence_length]\n",
    "\n",
    "            _, preds_phase = torch.max(outputs_phase.data, 1)\n",
    "            loss_phase = criterion_phase(outputs_phase, labels_phase)\n",
    "\n",
    "            loss = loss_phase\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss_phase += loss_phase.data.item()\n",
    "            train_loss_phase += loss_phase.data.item()\n",
    "\n",
    "            batch_corrects_phase = torch.sum(preds_phase == labels_phase.data)\n",
    "            train_corrects_phase += batch_corrects_phase\n",
    "\n",
    "\n",
    "            if (i+1)*train_batch_size >= num_train_all:               \n",
    "                running_loss_phase = 0.0\n",
    "                minibatch_correct_phase = 0.0\n",
    "\n",
    "            batch_progress += 1\n",
    "            if batch_progress*train_batch_size >= num_train_all:\n",
    "                percent = 100.0\n",
    "                print('Train progress: %s [%d/%d]' % (str(percent) + '%', num_train_all, num_train_all), end='\\n')\n",
    "            else:\n",
    "                percent = round(batch_progress*train_batch_size / num_train_all * 100, 2)\n",
    "                print('Train progress: %s [%d/%d]' % (str(percent) + '%', batch_progress*train_batch_size, num_train_all), end='\\r')\n",
    "\n",
    "        train_elapsed_time = time.time() - train_start_time\n",
    "        train_accuracy_phase = float(train_corrects_phase) / float(num_train_all) * sequence_length\n",
    "        train_average_loss_phase = train_loss_phase / num_train_all * sequence_length\n",
    "\n",
    "        \n",
    "        writer.add_scalar('train acc epoch phase',\n",
    "                          float(train_accuracy_phase),epoch)\n",
    "        writer.add_scalar('train loss epoch phase',\n",
    "                          float(train_average_loss_phase),epoch)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #  in evaluation mode.\n",
    "        model.eval()\n",
    "        val_loss_phase = 0.0\n",
    "        val_corrects_phase = 0\n",
    "        val_start_time = time.time()\n",
    "        val_progress = 0\n",
    "        val_all_preds_phase = []\n",
    "        val_all_labels_phase = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data in val_loader:\n",
    "\n",
    "                inputs, labels_phase = data[0].to(device), data[1].to(device)\n",
    "\n",
    "\n",
    "                labels_phase = labels_phase[(sequence_length - 1)::sequence_length]\n",
    "\n",
    "                \n",
    "                start_index_list = data[2]\n",
    "                start_index_list = start_index_list[0::sequence_length]\n",
    "                long_feature = get_long_feature(start_index_list=start_index_list,\n",
    "                                                dict_start_idx_LFB=dict_val_start_idx_LFB,\n",
    "                                                lfb=g_LFB_val)\n",
    "\n",
    "                long_feature = torch.Tensor(long_feature).to(device)\n",
    "\n",
    "                inputs = inputs.view(-1, sequence_length, 3, 216,216)\n",
    "                outputs_phase = model.forward(inputs, long_feature=long_feature)\n",
    "                \n",
    "                \n",
    "                # outputs_phase = outputs_phase[sequence_length - 1::sequence_length]\n",
    "\n",
    "                _, preds_phase = torch.max(outputs_phase.data, 1)\n",
    "                loss_phase = criterion_phase(outputs_phase, labels_phase)\n",
    "\n",
    "                val_loss_phase += loss_phase.data.item()\n",
    "\n",
    "                val_corrects_phase += torch.sum(preds_phase == labels_phase.data)\n",
    "\n",
    "\n",
    "                for i in range(len(preds_phase)):\n",
    "                    val_all_preds_phase.append(int(preds_phase.data.cpu()[i]))\n",
    "                for i in range(len(labels_phase)):\n",
    "                    val_all_labels_phase.append(int(labels_phase.data.cpu()[i]))\n",
    "\n",
    "\n",
    "                val_progress += 1\n",
    "                if val_progress*val_batch_size >= num_val_all:\n",
    "                    percent = 100.0\n",
    "                    print('Val progress: %s [%d/%d]' % (str(percent) + '%', num_val_all, num_val_all), end='\\n')\n",
    "                else:\n",
    "                    percent = round(val_progress*val_batch_size / num_val_all * 100, 2)\n",
    "                    print('Val progress: %s [%d/%d]' % (str(percent) + '%', val_progress*val_batch_size, num_val_all), end='\\r')\n",
    "\n",
    "        val_elapsed_time = time.time() - val_start_time\n",
    "        val_accuracy_phase = float(val_corrects_phase) / float(num_val_we_use)\n",
    "        val_average_loss_phase = val_loss_phase / num_val_we_use\n",
    "\n",
    "\n",
    "        writer.add_scalar('validation acc epoch phase',\n",
    "                          float(val_accuracy_phase),epoch)\n",
    "        writer.add_scalar('validation loss epoch phase',\n",
    "                          float(val_average_loss_phase),epoch)\n",
    "\n",
    "        print('epoch: {:4d}'\n",
    "              ' train in: {:2.0f}m{:2.0f}s'\n",
    "              ' train loss(phase): {:4.4f}'\n",
    "              ' train accu(phase): {:.4f}'\n",
    "              ' valid in: {:2.0f}m{:2.0f}s'\n",
    "              ' valid loss(phase): {:4.4f}'\n",
    "              ' valid accu(phase): {:.4f}'\n",
    "              .format(epoch,\n",
    "                      train_elapsed_time // 60,\n",
    "                      train_elapsed_time % 60,\n",
    "                      train_average_loss_phase,\n",
    "                      train_accuracy_phase,\n",
    "                      val_elapsed_time // 60,\n",
    "                      val_elapsed_time % 60,\n",
    "                      val_average_loss_phase,\n",
    "                      val_accuracy_phase))\n",
    "\n",
    "\n",
    "        # choose the best model by accuracy\n",
    "        if val_accuracy_phase > best_val_accuracy_phase:\n",
    "            best_val_accuracy_phase = val_accuracy_phase\n",
    "            correspond_train_acc_phase = train_accuracy_phase\n",
    "            #copy the best model\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            best_epoch = epoch\n",
    "        if val_accuracy_phase == best_val_accuracy_phase:\n",
    "            if train_accuracy_phase > correspond_train_acc_phase:\n",
    "                correspond_train_acc_phase = train_accuracy_phase\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                best_epoch = epoch\n",
    "\n",
    "        save_val_phase = int(\"{:4.0f}\".format(best_val_accuracy_phase * 10000))\n",
    "        save_train_phase = int(\"{:4.0f}\".format(correspond_train_acc_phase * 10000))\n",
    "        base_name = \"lstm\" \\\n",
    "                     + \"_epoch_\" + str(best_epoch) \\\n",
    "                     + \"_length_\" + str(sequence_length) \\\n",
    "                     + \"_batch_\" + str(train_batch_size) \\\n",
    "                     + \"_train_\" + str(save_train_phase) \\\n",
    "                     + \"_val_\" + str(save_val_phase)\n",
    "        \n",
    "        #model_save_path = 'sl10_flip1_lr5e-5/'\n",
    "        \n",
    "        torch.save(best_model_wts, \"/media/yilin/catarcnet/best_model/\"+model_save_path+base_name+\".pth\")\n",
    "        print(\"best_epoch\",str(best_epoch))\n",
    "        # model.module.state_dict()\n",
    "        \n",
    "        torch.save(model.state_dict(), \"/media/yilin/catarcnet/temp/\"+model_save_path+ \"latest_model_\"+str(epoch)+\".pth\")\n",
    "              \n",
    "\n",
    "    return \"Complete\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05138656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this experiment, train06-train25 for training, train01-train05 for validation\n",
    "\n",
    "train_dataset_20, train_num_each_20, \\\n",
    "val_dataset_5, val_num_each_5 = get_dataset('../../gen_datasets/train_val_paths_labels.pkl')\n",
    "\n",
    "train_model(MODEL,(train_dataset_20),(train_num_each_20),(val_dataset_20),(val_num_each_20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c7bf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# before training, we need make a dir of the best model and temp model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2fa73e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff69fb5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b43cef2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_paths_20  :   1456\n",
      "train_labels_20 :   1456\n",
      "valid_paths_5  :    867\n",
      "valid_labels_5 :    867\n"
     ]
    }
   ],
   "source": [
    "# for show example output of train process\n",
    "# 3 training videos, 2 val videos\n",
    "train_dataset_5, train_num_each_5, \\\n",
    "val_dataset_5, val_num_each_5 = get_dataset('../../gen_datasets/train_val_paths_labels_3.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1a9e01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54a8d4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num train start idx :   1429\n",
      "num of all train use:  14290\n",
      "num of all valid use:   8490\n",
      "num of all train LFB use:  14290\n",
      "num of all valid LFB use:   8490\n",
      "loading features!->.........\n",
      "train feature length: 10\n",
      "train feature length: 20\n",
      "train feature length: 30\n",
      "train feature length: 40\n",
      "train feature length: 50\n",
      "train feature length: 60\n",
      "train feature length: 70\n",
      "train feature length: 80\n",
      "train feature length: 90\n",
      "train feature length: 100\n",
      "train feature length: 110\n",
      "train feature length: 120\n",
      "train feature length: 130\n",
      "train feature length: 140\n",
      "train feature length: 150\n",
      "train feature length: 160\n",
      "train feature length: 170\n",
      "train feature length: 180\n",
      "train feature length: 190\n",
      "train feature length: 200\n",
      "train feature length: 210\n",
      "train feature length: 220\n",
      "train feature length: 230\n",
      "train feature length: 240\n",
      "train feature length: 250\n",
      "train feature length: 260\n",
      "train feature length: 270\n",
      "train feature length: 280\n",
      "train feature length: 290\n",
      "train feature length: 300\n",
      "train feature length: 310\n",
      "train feature length: 320\n",
      "train feature length: 330\n",
      "train feature length: 340\n",
      "train feature length: 350\n",
      "train feature length: 360\n",
      "train feature length: 370\n",
      "train feature length: 380\n",
      "train feature length: 390\n",
      "train feature length: 400\n",
      "train feature length: 410\n",
      "train feature length: 420\n",
      "train feature length: 430\n",
      "train feature length: 440\n",
      "train feature length: 450\n",
      "train feature length: 460\n",
      "train feature length: 470\n",
      "train feature length: 480\n",
      "train feature length: 490\n",
      "train feature length: 500\n",
      "train feature length: 510\n",
      "train feature length: 520\n",
      "train feature length: 530\n",
      "train feature length: 540\n",
      "train feature length: 550\n",
      "train feature length: 560\n",
      "train feature length: 570\n",
      "train feature length: 580\n",
      "train feature length: 590\n",
      "train feature length: 600\n",
      "train feature length: 610\n",
      "train feature length: 620\n",
      "train feature length: 630\n",
      "train feature length: 640\n",
      "train feature length: 650\n",
      "train feature length: 660\n",
      "train feature length: 670\n",
      "train feature length: 680\n",
      "train feature length: 690\n",
      "train feature length: 700\n",
      "train feature length: 710\n",
      "train feature length: 720\n",
      "train feature length: 730\n",
      "train feature length: 740\n",
      "train feature length: 750\n",
      "train feature length: 760\n",
      "train feature length: 770\n",
      "train feature length: 780\n",
      "train feature length: 790\n",
      "train feature length: 800\n",
      "train feature length: 810\n",
      "train feature length: 820\n",
      "train feature length: 830\n",
      "train feature length: 840\n",
      "train feature length: 850\n",
      "train feature length: 860\n",
      "train feature length: 870\n",
      "train feature length: 880\n",
      "train feature length: 890\n",
      "train feature length: 900\n",
      "train feature length: 910\n",
      "train feature length: 920\n",
      "train feature length: 930\n",
      "train feature length: 940\n",
      "train feature length: 950\n",
      "train feature length: 960\n",
      "train feature length: 970\n",
      "train feature length: 980\n",
      "train feature length: 990\n",
      "train feature length: 1000\n",
      "train feature length: 1010\n",
      "train feature length: 1020\n",
      "train feature length: 1030\n",
      "train feature length: 1040\n",
      "train feature length: 1050\n",
      "train feature length: 1060\n",
      "train feature length: 1070\n",
      "train feature length: 1080\n",
      "train feature length: 1090\n",
      "train feature length: 1100\n",
      "train feature length: 1110\n",
      "train feature length: 1120\n",
      "train feature length: 1130\n",
      "train feature length: 1140\n",
      "train feature length: 1150\n",
      "train feature length: 1160\n",
      "train feature length: 1170\n",
      "train feature length: 1180\n",
      "train feature length: 1190\n",
      "train feature length: 1200\n",
      "train feature length: 1210\n",
      "train feature length: 1220\n",
      "train feature length: 1230\n",
      "train feature length: 1240\n",
      "train feature length: 1250\n",
      "train feature length: 1260\n",
      "train feature length: 1270\n",
      "train feature length: 1280\n",
      "train feature length: 1290\n",
      "train feature length: 1300\n",
      "train feature length: 1310\n",
      "train feature length: 1320\n",
      "train feature length: 1330\n",
      "train feature length: 1340\n",
      "train feature length: 1350\n",
      "train feature length: 1360\n",
      "train feature length: 1370\n",
      "train feature length: 1380\n",
      "train feature length: 1390\n",
      "train feature length: 1400\n",
      "train feature length: 1410\n",
      "train feature length: 1420\n",
      "train feature length: 1429\n",
      "val feature length: 10\n",
      "val feature length: 20\n",
      "val feature length: 30\n",
      "val feature length: 40\n",
      "val feature length: 50\n",
      "val feature length: 60\n",
      "val feature length: 70\n",
      "val feature length: 80\n",
      "val feature length: 90\n",
      "val feature length: 100\n",
      "val feature length: 110\n",
      "val feature length: 120\n",
      "val feature length: 130\n",
      "val feature length: 140\n",
      "val feature length: 150\n",
      "val feature length: 160\n",
      "val feature length: 170\n",
      "val feature length: 180\n",
      "val feature length: 190\n",
      "val feature length: 200\n",
      "val feature length: 210\n",
      "val feature length: 220\n",
      "val feature length: 230\n",
      "val feature length: 240\n",
      "val feature length: 250\n",
      "val feature length: 260\n",
      "val feature length: 270\n",
      "val feature length: 280\n",
      "val feature length: 290\n",
      "val feature length: 300\n",
      "val feature length: 310\n",
      "val feature length: 320\n",
      "val feature length: 330\n",
      "val feature length: 340\n",
      "val feature length: 350\n",
      "val feature length: 360\n",
      "val feature length: 370\n",
      "val feature length: 380\n",
      "val feature length: 390\n",
      "val feature length: 400\n",
      "val feature length: 410\n",
      "val feature length: 420\n",
      "val feature length: 430\n",
      "val feature length: 440\n",
      "val feature length: 450\n",
      "val feature length: 460\n",
      "val feature length: 470\n",
      "val feature length: 480\n",
      "val feature length: 490\n",
      "val feature length: 500\n",
      "val feature length: 510\n",
      "val feature length: 520\n",
      "val feature length: 530\n",
      "val feature length: 540\n",
      "val feature length: 550\n",
      "val feature length: 560\n",
      "val feature length: 570\n",
      "val feature length: 580\n",
      "val feature length: 590\n",
      "val feature length: 600\n",
      "val feature length: 610\n",
      "val feature length: 620\n",
      "val feature length: 630\n",
      "val feature length: 640\n",
      "val feature length: 650\n",
      "val feature length: 660\n",
      "val feature length: 670\n",
      "val feature length: 680\n",
      "val feature length: 690\n",
      "val feature length: 700\n",
      "val feature length: 710\n",
      "val feature length: 720\n",
      "val feature length: 730\n",
      "val feature length: 740\n",
      "val feature length: 750\n",
      "val feature length: 760\n",
      "val feature length: 770\n",
      "val feature length: 780\n",
      "val feature length: 790\n",
      "val feature length: 800\n",
      "val feature length: 810\n",
      "val feature length: 820\n",
      "val feature length: 830\n",
      "val feature length: 840\n",
      "val feature length: 849\n",
      "finish!\n",
      "g_LFB_train shape: (1429, 512)\n",
      "g_LFB_val shape: (849, 512)\n",
      "Train progress: 100.0% [14290/14290]\n",
      "Val progress: 100.0% [8490/8490]\n",
      "epoch:    0 train in:  0m58s train loss(phase): 0.1731 train accu(phase): 0.4276 valid in:  0m11s valid loss(phase): 0.1118 valid accu(phase): 0.5230\n",
      "best_epoch 0\n",
      "Train progress: 100.0% [14290/14290]\n",
      "Val progress: 100.0% [8490/8490]\n",
      "epoch:    1 train in:  0m58s train loss(phase): 0.0557 train accu(phase): 0.7222 valid in:  0m11s valid loss(phase): 0.1047 valid accu(phase): 0.5559\n",
      "best_epoch 1\n",
      "Train progress: 100.0% [14290/14290]\n",
      "Val progress: 100.0% [8490/8490]\n",
      "epoch:    2 train in:  0m59s train loss(phase): 0.0440 train accu(phase): 0.7691 valid in:  0m11s valid loss(phase): 0.0857 valid accu(phase): 0.6843\n",
      "best_epoch 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Complete'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the first time to train, it is necessary to load feature bank\n",
    "\n",
    "sequence_length = 10    # the length of input clip \n",
    "train_batch_size = 100  # batch size \n",
    "val_batch_size = 100     \n",
    "epochs = 3 \n",
    "workers = 2   \n",
    "learning_rate = 5e-5\n",
    "LFB_length = 30\n",
    "load_exist_LFB = True # False\n",
    "\n",
    "MODEL = M4_resnet_lstm_nl() # M5_densenet_lstm_nl() /  M6_resnest_lstm_nl()\n",
    "\n",
    "model_LFB = LFB_resnet_lstm()\n",
    "LFB_train_path_save_path = \"./LFB/g_LFB_train_resnet.pkl\"\n",
    "LFB_val_path_save_path = \"./LFB/g_LFB_val_resnet.pkl\"\n",
    "\n",
    "\n",
    "\n",
    "#########################################\n",
    "device = \"cuda:2\"  \n",
    "\n",
    "model_save_path = 'non-local/pretrained_lr1e-5_L30_2fc_resnet_nl/' # 'resnet/sl10_lr5e-5_6-25train/' cuda:0\n",
    "tensorboard_path = 'runs/' + model_save_path\n",
    "\n",
    "LFB_path = \"/media/yilin/catarcnet/temp/resnet/sl10_lr5e-5_6-25train/latest_model_14.pth\"\n",
    "###########################################\n",
    "\n",
    "\n",
    "# train M4  # example output using train01-03 for training, train04-train05 for validation\n",
    "train_model(MODEL,(train_dataset_5),(train_num_each_5),(val_dataset_5),(val_num_each_5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267a286f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1fbcc733",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num train start idx :   1429\n",
      "num of all train use:  14290\n",
      "num of all valid use:   8490\n",
      "num of all train LFB use:  14290\n",
      "num of all valid LFB use:   8490\n",
      "loading features!->.........\n",
      "load completed\n",
      "g_LFB_train shape: (1429, 512)\n",
      "g_LFB_val shape: (849, 512)\n",
      "Train progress: 100.0% [14290/14290]\n",
      "Val progress: 100.0% [8490/8490]\n",
      "epoch:    0 train in:  0m58s train loss(phase): 0.1670 train accu(phase): 0.4430 valid in:  0m11s valid loss(phase): 0.1155 valid accu(phase): 0.5112\n",
      "best_epoch 0\n",
      "Train progress: 100.0% [14290/14290]\n",
      "Val progress: 100.0% [8490/8490]\n",
      "epoch:    1 train in:  0m58s train loss(phase): 0.0582 train accu(phase): 0.7124 valid in:  0m11s valid loss(phase): 0.0940 valid accu(phase): 0.6160\n",
      "best_epoch 1\n",
      "Train progress: 100.0% [14290/14290]\n",
      "Val progress: 100.0% [8490/8490]\n",
      "epoch:    2 train in:  0m58s train loss(phase): 0.0370 train accu(phase): 0.7873 valid in:  0m12s valid loss(phase): 0.1253 valid accu(phase): 0.5524\n",
      "best_epoch 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Complete'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the first time to train, it is necessary to load feature bank\n",
    "\n",
    "sequence_length = 10    # the length of input clip \n",
    "train_batch_size = 100  # batch size \n",
    "val_batch_size = 100     \n",
    "epochs = 3 \n",
    "workers = 2   \n",
    "learning_rate = 5e-5\n",
    "LFB_length = 30\n",
    "load_exist_LFB = True # False\n",
    "\n",
    "MODEL = M4_resnet_lstm_nl() # M5_densenet_lstm_nl() /  M6_resnest_lstm_nl()\n",
    "\n",
    "model_LFB = LFB_resnet_lstm()\n",
    "LFB_train_path_save_path = \"./LFB/g_LFB_train_resnet.pkl\"\n",
    "LFB_val_path_save_path = \"./LFB/g_LFB_val_resnet.pkl\"\n",
    "\n",
    "\n",
    "\n",
    "#########################################\n",
    "device = \"cuda:2\"  \n",
    "\n",
    "model_save_path = 'non-local/pretrained_lr5e-5_L30_2fc_resnet_nl/' # 'resnet/sl10_lr5e-5_6-25train/' cuda:0\n",
    "tensorboard_path = 'runs/' + model_save_path\n",
    "\n",
    "LFB_path = \"/media/yilin/catarcnet/temp/resnet/sl10_lr5e-5_6-25train/latest_model_14.pth\"\n",
    "###########################################\n",
    "\n",
    "\n",
    "# train M4\n",
    "train_model(MODEL,(train_dataset_5),(train_num_each_5),(val_dataset_5),(val_num_each_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fa2bad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3d393bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num train start idx :   1429\n",
      "num of all train use:  14290\n",
      "num of all valid use:   8490\n",
      "num of all train LFB use:  14290\n",
      "num of all valid LFB use:   8490\n",
      "loading features!->.........\n",
      "train feature length: 10\n",
      "train feature length: 20\n",
      "train feature length: 30\n",
      "train feature length: 40\n",
      "train feature length: 50\n",
      "train feature length: 60\n",
      "train feature length: 70\n",
      "train feature length: 80\n",
      "train feature length: 90\n",
      "train feature length: 100\n",
      "train feature length: 110\n",
      "train feature length: 120\n",
      "train feature length: 130\n",
      "train feature length: 140\n",
      "train feature length: 150\n",
      "train feature length: 160\n",
      "train feature length: 170\n",
      "train feature length: 180\n",
      "train feature length: 190\n",
      "train feature length: 200\n",
      "train feature length: 210\n",
      "train feature length: 220\n",
      "train feature length: 230\n",
      "train feature length: 240\n",
      "train feature length: 250\n",
      "train feature length: 260\n",
      "train feature length: 270\n",
      "train feature length: 280\n",
      "train feature length: 290\n",
      "train feature length: 300\n",
      "train feature length: 310\n",
      "train feature length: 320\n",
      "train feature length: 330\n",
      "train feature length: 340\n",
      "train feature length: 350\n",
      "train feature length: 360\n",
      "train feature length: 370\n",
      "train feature length: 380\n",
      "train feature length: 390\n",
      "train feature length: 400\n",
      "train feature length: 410\n",
      "train feature length: 420\n",
      "train feature length: 430\n",
      "train feature length: 440\n",
      "train feature length: 450\n",
      "train feature length: 460\n",
      "train feature length: 470\n",
      "train feature length: 480\n",
      "train feature length: 490\n",
      "train feature length: 500\n",
      "train feature length: 510\n",
      "train feature length: 520\n",
      "train feature length: 530\n",
      "train feature length: 540\n",
      "train feature length: 550\n",
      "train feature length: 560\n",
      "train feature length: 570\n",
      "train feature length: 580\n",
      "train feature length: 590\n",
      "train feature length: 600\n",
      "train feature length: 610\n",
      "train feature length: 620\n",
      "train feature length: 630\n",
      "train feature length: 640\n",
      "train feature length: 650\n",
      "train feature length: 660\n",
      "train feature length: 670\n",
      "train feature length: 680\n",
      "train feature length: 690\n",
      "train feature length: 700\n",
      "train feature length: 710\n",
      "train feature length: 720\n",
      "train feature length: 730\n",
      "train feature length: 740\n",
      "train feature length: 750\n",
      "train feature length: 760\n",
      "train feature length: 770\n",
      "train feature length: 780\n",
      "train feature length: 790\n",
      "train feature length: 800\n",
      "train feature length: 810\n",
      "train feature length: 820\n",
      "train feature length: 830\n",
      "train feature length: 840\n",
      "train feature length: 850\n",
      "train feature length: 860\n",
      "train feature length: 870\n",
      "train feature length: 880\n",
      "train feature length: 890\n",
      "train feature length: 900\n",
      "train feature length: 910\n",
      "train feature length: 920\n",
      "train feature length: 930\n",
      "train feature length: 940\n",
      "train feature length: 950\n",
      "train feature length: 960\n",
      "train feature length: 970\n",
      "train feature length: 980\n",
      "train feature length: 990\n",
      "train feature length: 1000\n",
      "train feature length: 1010\n",
      "train feature length: 1020\n",
      "train feature length: 1030\n",
      "train feature length: 1040\n",
      "train feature length: 1050\n",
      "train feature length: 1060\n",
      "train feature length: 1070\n",
      "train feature length: 1080\n",
      "train feature length: 1090\n",
      "train feature length: 1100\n",
      "train feature length: 1110\n",
      "train feature length: 1120\n",
      "train feature length: 1130\n",
      "train feature length: 1140\n",
      "train feature length: 1150\n",
      "train feature length: 1160\n",
      "train feature length: 1170\n",
      "train feature length: 1180\n",
      "train feature length: 1190\n",
      "train feature length: 1200\n",
      "train feature length: 1210\n",
      "train feature length: 1220\n",
      "train feature length: 1230\n",
      "train feature length: 1240\n",
      "train feature length: 1250\n",
      "train feature length: 1260\n",
      "train feature length: 1270\n",
      "train feature length: 1280\n",
      "train feature length: 1290\n",
      "train feature length: 1300\n",
      "train feature length: 1310\n",
      "train feature length: 1320\n",
      "train feature length: 1330\n",
      "train feature length: 1340\n",
      "train feature length: 1350\n",
      "train feature length: 1360\n",
      "train feature length: 1370\n",
      "train feature length: 1380\n",
      "train feature length: 1390\n",
      "train feature length: 1400\n",
      "train feature length: 1410\n",
      "train feature length: 1420\n",
      "train feature length: 1429\n",
      "val feature length: 10\n",
      "val feature length: 20\n",
      "val feature length: 30\n",
      "val feature length: 40\n",
      "val feature length: 50\n",
      "val feature length: 60\n",
      "val feature length: 70\n",
      "val feature length: 80\n",
      "val feature length: 90\n",
      "val feature length: 100\n",
      "val feature length: 110\n",
      "val feature length: 120\n",
      "val feature length: 130\n",
      "val feature length: 140\n",
      "val feature length: 150\n",
      "val feature length: 160\n",
      "val feature length: 170\n",
      "val feature length: 180\n",
      "val feature length: 190\n",
      "val feature length: 200\n",
      "val feature length: 210\n",
      "val feature length: 220\n",
      "val feature length: 230\n",
      "val feature length: 240\n",
      "val feature length: 250\n",
      "val feature length: 260\n",
      "val feature length: 270\n",
      "val feature length: 280\n",
      "val feature length: 290\n",
      "val feature length: 300\n",
      "val feature length: 310\n",
      "val feature length: 320\n",
      "val feature length: 330\n",
      "val feature length: 340\n",
      "val feature length: 350\n",
      "val feature length: 360\n",
      "val feature length: 370\n",
      "val feature length: 380\n",
      "val feature length: 390\n",
      "val feature length: 400\n",
      "val feature length: 410\n",
      "val feature length: 420\n",
      "val feature length: 430\n",
      "val feature length: 440\n",
      "val feature length: 450\n",
      "val feature length: 460\n",
      "val feature length: 470\n",
      "val feature length: 480\n",
      "val feature length: 490\n",
      "val feature length: 500\n",
      "val feature length: 510\n",
      "val feature length: 520\n",
      "val feature length: 530\n",
      "val feature length: 540\n",
      "val feature length: 550\n",
      "val feature length: 560\n",
      "val feature length: 570\n",
      "val feature length: 580\n",
      "val feature length: 590\n",
      "val feature length: 600\n",
      "val feature length: 610\n",
      "val feature length: 620\n",
      "val feature length: 630\n",
      "val feature length: 640\n",
      "val feature length: 650\n",
      "val feature length: 660\n",
      "val feature length: 670\n",
      "val feature length: 680\n",
      "val feature length: 690\n",
      "val feature length: 700\n",
      "val feature length: 710\n",
      "val feature length: 720\n",
      "val feature length: 730\n",
      "val feature length: 740\n",
      "val feature length: 750\n",
      "val feature length: 760\n",
      "val feature length: 770\n",
      "val feature length: 780\n",
      "val feature length: 790\n",
      "val feature length: 800\n",
      "val feature length: 810\n",
      "val feature length: 820\n",
      "val feature length: 830\n",
      "val feature length: 840\n",
      "val feature length: 849\n",
      "finish!\n",
      "g_LFB_train shape: (1429, 512)\n",
      "g_LFB_val shape: (849, 512)\n",
      "Train progress: 100.0% [14290/14290]\n",
      "Val progress: 100.0% [8490/8490]\n",
      "epoch:    0 train in:  1m 9s train loss(phase): 0.2203 train accu(phase): 0.3051 valid in:  0m13s valid loss(phase): 0.1929 valid accu(phase): 0.3451\n",
      "best_epoch 0\n",
      "Train progress: 100.0% [14290/14290]\n",
      "Val progress: 100.0% [8490/8490]\n",
      "epoch:    1 train in:  1m 9s train loss(phase): 0.1056 train accu(phase): 0.6186 valid in:  0m13s valid loss(phase): 0.1476 valid accu(phase): 0.4865\n",
      "best_epoch 1\n",
      "Train progress: 100.0% [14290/14290]\n",
      "Val progress: 100.0% [8490/8490]\n",
      "epoch:    2 train in:  1m 9s train loss(phase): 0.0714 train accu(phase): 0.7166 valid in:  0m14s valid loss(phase): 0.1263 valid accu(phase): 0.5642\n",
      "best_epoch 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Complete'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_length = 10    # the length of input clip \n",
    "train_batch_size = 100  # batch size \n",
    "val_batch_size = 100     \n",
    "epochs = 3 \n",
    "workers = 2   \n",
    "learning_rate = 1e-5\n",
    "LFB_length = 30\n",
    "load_exist_LFB = False # False\n",
    "\n",
    "MODEL = M5_densenet_lstm_nl() # M5_densenet_lstm_nl() /  M6_resnest_lstm_nl()\n",
    "\n",
    "model_LFB = LFB_densenet_lstm()\n",
    "LFB_train_path_save_path = \"./LFB/g_LFB_train_densenet.pkl\"\n",
    "LFB_val_path_save_path = \"./LFB/g_LFB_val_densenet.pkl\"\n",
    "\n",
    "\n",
    "\n",
    "#########################################\n",
    "device = \"cuda:2\"  \n",
    "\n",
    "model_save_path = 'non-local/pretrained_lr1e-5_L30_2fc_densenet_nl/' # 'resnet/sl10_lr5e-5_6-25train/' cuda:0\n",
    "tensorboard_path = 'runs/' + model_save_path\n",
    "\n",
    "LFB_path = \"/media/yilin/catarcnet/temp/densenet/sl10_lr1e-5_6-25train/latest_model_6.pth\"\n",
    "###########################################\n",
    "\n",
    "\n",
    "# train M5\n",
    "train_model(MODEL,(train_dataset_5),(train_num_each_5),(val_dataset_5),(val_num_each_5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237d7b73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ea30a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num train start idx :   1429\n",
      "num of all train use:  14290\n",
      "num of all valid use:   8490\n",
      "num of all train LFB use:  14290\n",
      "num of all valid LFB use:   8490\n",
      "loading features!->.........\n",
      "train feature length: 10\n",
      "train feature length: 20\n",
      "train feature length: 30\n",
      "train feature length: 40\n",
      "train feature length: 50\n",
      "train feature length: 60\n",
      "train feature length: 70\n",
      "train feature length: 80\n",
      "train feature length: 90\n",
      "train feature length: 100\n",
      "train feature length: 110\n",
      "train feature length: 120\n",
      "train feature length: 130\n",
      "train feature length: 140\n",
      "train feature length: 150\n",
      "train feature length: 160\n",
      "train feature length: 170\n",
      "train feature length: 180\n",
      "train feature length: 190\n",
      "train feature length: 200\n",
      "train feature length: 210\n",
      "train feature length: 220\n",
      "train feature length: 230\n",
      "train feature length: 240\n",
      "train feature length: 250\n",
      "train feature length: 260\n",
      "train feature length: 270\n",
      "train feature length: 280\n",
      "train feature length: 290\n",
      "train feature length: 300\n",
      "train feature length: 310\n",
      "train feature length: 320\n",
      "train feature length: 330\n",
      "train feature length: 340\n",
      "train feature length: 350\n",
      "train feature length: 360\n",
      "train feature length: 370\n",
      "train feature length: 380\n",
      "train feature length: 390\n",
      "train feature length: 400\n",
      "train feature length: 410\n",
      "train feature length: 420\n",
      "train feature length: 430\n",
      "train feature length: 440\n",
      "train feature length: 450\n",
      "train feature length: 460\n",
      "train feature length: 470\n",
      "train feature length: 480\n",
      "train feature length: 490\n",
      "train feature length: 500\n",
      "train feature length: 510\n",
      "train feature length: 520\n",
      "train feature length: 530\n",
      "train feature length: 540\n",
      "train feature length: 550\n",
      "train feature length: 560\n",
      "train feature length: 570\n",
      "train feature length: 580\n",
      "train feature length: 590\n",
      "train feature length: 600\n",
      "train feature length: 610\n",
      "train feature length: 620\n",
      "train feature length: 630\n",
      "train feature length: 640\n",
      "train feature length: 650\n",
      "train feature length: 660\n",
      "train feature length: 670\n",
      "train feature length: 680\n",
      "train feature length: 690\n",
      "train feature length: 700\n",
      "train feature length: 710\n",
      "train feature length: 720\n",
      "train feature length: 730\n",
      "train feature length: 740\n",
      "train feature length: 750\n",
      "train feature length: 760\n",
      "train feature length: 770\n",
      "train feature length: 780\n",
      "train feature length: 790\n",
      "train feature length: 800\n",
      "train feature length: 810\n",
      "train feature length: 820\n",
      "train feature length: 830\n",
      "train feature length: 840\n",
      "train feature length: 850\n",
      "train feature length: 860\n",
      "train feature length: 870\n",
      "train feature length: 880\n",
      "train feature length: 890\n",
      "train feature length: 900\n",
      "train feature length: 910\n",
      "train feature length: 920\n",
      "train feature length: 930\n",
      "train feature length: 940\n",
      "train feature length: 950\n",
      "train feature length: 960\n",
      "train feature length: 970\n",
      "train feature length: 980\n",
      "train feature length: 990\n",
      "train feature length: 1000\n",
      "train feature length: 1010\n",
      "train feature length: 1020\n",
      "train feature length: 1030\n",
      "train feature length: 1040\n",
      "train feature length: 1050\n",
      "train feature length: 1060\n",
      "train feature length: 1070\n",
      "train feature length: 1080\n",
      "train feature length: 1090\n",
      "train feature length: 1100\n",
      "train feature length: 1110\n",
      "train feature length: 1120\n",
      "train feature length: 1130\n",
      "train feature length: 1140\n",
      "train feature length: 1150\n",
      "train feature length: 1160\n",
      "train feature length: 1170\n",
      "train feature length: 1180\n",
      "train feature length: 1190\n",
      "train feature length: 1200\n",
      "train feature length: 1210\n",
      "train feature length: 1220\n",
      "train feature length: 1230\n",
      "train feature length: 1240\n",
      "train feature length: 1250\n",
      "train feature length: 1260\n",
      "train feature length: 1270\n",
      "train feature length: 1280\n",
      "train feature length: 1290\n",
      "train feature length: 1300\n",
      "train feature length: 1310\n",
      "train feature length: 1320\n",
      "train feature length: 1330\n",
      "train feature length: 1340\n",
      "train feature length: 1350\n",
      "train feature length: 1360\n",
      "train feature length: 1370\n",
      "train feature length: 1380\n",
      "train feature length: 1390\n",
      "train feature length: 1400\n",
      "train feature length: 1410\n",
      "train feature length: 1420\n",
      "train feature length: 1429\n",
      "val feature length: 10\n",
      "val feature length: 20\n",
      "val feature length: 30\n",
      "val feature length: 40\n",
      "val feature length: 50\n",
      "val feature length: 60\n",
      "val feature length: 70\n",
      "val feature length: 80\n",
      "val feature length: 90\n",
      "val feature length: 100\n",
      "val feature length: 110\n",
      "val feature length: 120\n",
      "val feature length: 130\n",
      "val feature length: 140\n",
      "val feature length: 150\n",
      "val feature length: 160\n",
      "val feature length: 170\n",
      "val feature length: 180\n",
      "val feature length: 190\n",
      "val feature length: 200\n",
      "val feature length: 210\n",
      "val feature length: 220\n",
      "val feature length: 230\n",
      "val feature length: 240\n",
      "val feature length: 250\n",
      "val feature length: 260\n",
      "val feature length: 270\n",
      "val feature length: 280\n",
      "val feature length: 290\n",
      "val feature length: 300\n",
      "val feature length: 310\n",
      "val feature length: 320\n",
      "val feature length: 330\n",
      "val feature length: 340\n",
      "val feature length: 350\n",
      "val feature length: 360\n",
      "val feature length: 370\n",
      "val feature length: 380\n",
      "val feature length: 390\n",
      "val feature length: 400\n",
      "val feature length: 410\n",
      "val feature length: 420\n",
      "val feature length: 430\n",
      "val feature length: 440\n",
      "val feature length: 450\n",
      "val feature length: 460\n",
      "val feature length: 470\n",
      "val feature length: 480\n",
      "val feature length: 490\n",
      "val feature length: 500\n",
      "val feature length: 510\n",
      "val feature length: 520\n",
      "val feature length: 530\n",
      "val feature length: 540\n",
      "val feature length: 550\n",
      "val feature length: 560\n",
      "val feature length: 570\n",
      "val feature length: 580\n",
      "val feature length: 590\n",
      "val feature length: 600\n",
      "val feature length: 610\n",
      "val feature length: 620\n",
      "val feature length: 630\n",
      "val feature length: 640\n",
      "val feature length: 650\n",
      "val feature length: 660\n",
      "val feature length: 670\n",
      "val feature length: 680\n",
      "val feature length: 690\n",
      "val feature length: 700\n",
      "val feature length: 710\n",
      "val feature length: 720\n",
      "val feature length: 730\n",
      "val feature length: 740\n",
      "val feature length: 750\n",
      "val feature length: 760\n",
      "val feature length: 770\n",
      "val feature length: 780\n",
      "val feature length: 790\n",
      "val feature length: 800\n",
      "val feature length: 810\n",
      "val feature length: 820\n",
      "val feature length: 830\n",
      "val feature length: 840\n",
      "val feature length: 849\n",
      "finish!\n",
      "g_LFB_train shape: (1429, 512)\n",
      "g_LFB_val shape: (849, 512)\n",
      "Train progress: 100.0% [14290/14290]\n",
      "Val progress: 100.0% [8490/8490]\n",
      "epoch:    0 train in:  1m 9s train loss(phase): 0.1607 train accu(phase): 0.4577 valid in:  0m15s valid loss(phase): 0.1419 valid accu(phase): 0.3675\n",
      "best_epoch 0\n",
      "Train progress: 100.0% [14290/14290]\n",
      "Val progress: 100.0% [8490/8490]\n",
      "epoch:    1 train in:  1m10s train loss(phase): 0.0500 train accu(phase): 0.7376 valid in:  0m15s valid loss(phase): 0.1163 valid accu(phase): 0.5724\n",
      "best_epoch 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Complete'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_length = 10    # the length of input clip \n",
    "train_batch_size = 100  # batch size \n",
    "val_batch_size = 100     \n",
    "epochs = 2 \n",
    "workers = 2   \n",
    "learning_rate = 5e-5\n",
    "LFB_length = 30\n",
    "load_exist_LFB = False # True\n",
    "\n",
    "MODEL = M6_resnest_lstm_nl() # M5_densenet_lstm_nl() /  M6_resnest_lstm_nl()\n",
    "\n",
    "model_LFB = LFB_resnest_lstm()\n",
    "LFB_train_path_save_path = \"./LFB/g_LFB_train_resnest.pkl\"\n",
    "LFB_val_path_save_path = \"./LFB/g_LFB_val_resnest.pkl\"\n",
    "\n",
    "\n",
    "\n",
    "#########################################\n",
    "device = \"cuda:2\"  \n",
    "\n",
    "model_save_path = 'non-local/pretrained_lr1e-5_L30_2fc_resnest_nl/' # 'resnet/sl10_lr5e-5_6-25train/' cuda:0\n",
    "tensorboard_path = 'runs/' + model_save_path\n",
    "\n",
    "LFB_path = \"/media/yilin/catarcnet/temp/resnest/sl10_lr1e-5_6-25train/latest_model_12.pth\"\n",
    "###########################################\n",
    "\n",
    "\n",
    "# train M6\n",
    "train_model(MODEL,(train_dataset_5),(train_num_each_5),(val_dataset_5),(val_num_each_5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b383ac5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
